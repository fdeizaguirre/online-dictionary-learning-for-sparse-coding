{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentación\n",
    "En este jupyter notebook va a encontrar una implementación del algoritmo Online Dictionary Learning descrito en el paper [Online dictionary learning for sparse coding](https://dl.acm.org/doi/10.1145/1553374.1553463).Y cuya interpretación y un análisis resumido se puede encontrar en el documento de la primer entrega.\n",
    "Para esta implementación se utilizó como fuente de inspiración el código del siguiente repositorio [repositorio](https://github.com/MehdiAbbanaBennani/online-dictionary-learning-for-sparse-coding) y scripts que se encuentran en el [repositorio del curso] (https://gitlab.fing.edu.uy/tao/datos/-/tree/main?ref_type=heads), principalmente para el uso del dataset del [proyecto LUISA](https://mh.udelar.edu.uy/luisa/).\n",
    "\n",
    "Además de la implementación, se muestran los resultados para distintos parámetros y mejoras en el algoritmo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspirado en los repositorios\n",
    "# https://gitlab.fing.edu.uy/tao/datos\n",
    "# https://github.com/MehdiAbbanaBennani/online-dictionary-learning-for-sparse-coding/tree/master\n",
    "\n",
    "# %%\n",
    "import sys, os\n",
    "from itertools import tee\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, LassoLars\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "sys.path.append(os.path.abspath(\"/home/pancho/Documents/PhD/FING/TAO/entregable/datos\"))\n",
    "\n",
    "import datos\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datos.get_char_luisa()\n",
    "lambda_reg = 0.001  # Parametro de regularizacion\n",
    "k = 250  # Cantidad de átomos en el diccionario\n",
    "n_obs = len(data)\n",
    "dim_obs = len(data[0])\n",
    "log_step = 40  # Cada cuantas iteraciones almacenar los datos\n",
    "test_batch_size = 1000  # Cantidad de muestras para test\n",
    "losses = []\n",
    "regret = []\n",
    "offline_loss = []\n",
    "objective = []\n",
    "\n",
    "alphas = []\n",
    "observed = []\n",
    "cumulative_losses = []\n",
    "\n",
    "np.random.seed(14)  # semilla para hacer pruebas comparables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje de diccionarios\n",
    "En las siguientes secciones se mostrará los resultados que se obtienen al variar las inicializaciones de las matrices A, B y el diccionario D.  \n",
    "\n",
    "También se verá el impacto de cambiar entre los algoritmos de optimización Lasso y Lasso-LARS, para los cuales se muestran resultados utilizando distintos valores del parámetro de regularización lambda. En cuanto al tamaño del batch, se utilizó dos valores, se utilizaron batches de un único elemento para seguir el modo más \"puro\" del método y con batches de tamaño 200 elementos.\n",
    "\n",
    "## Sobre la cantidad de átomos en el diccionario\n",
    "En la mayoría de los ejemplos que se muestran, la cantidad de átomos del diccionario es 250. Esta cantidad fue seleccionada por capacidad computacional para poder realizar en un tiempo razonable las distintas pruebas. Se buscó la cantidad de átomos fuera amplia y a la vez que fuera analizable al mirar la imagen del diccionario aprendid. Esta condición a la vez que la cantidad de átomos sea mayor a la cantidad de letras del alfabeto, más la cantidad de números y de símbolos. Se especula que hay mayor cantidad de caracteres debido a que en los datos hay distintas \"fuente\", caracteres combinados, ejemplo aquellos que tienen tildes, etc. Dado que en estos ejemplos  la cantidad de  átomos no fue superior a la cantidad características, hay un último ejemplo con un diccionario de 1000 elementos.\n",
    "\n",
    "\n",
    "## Descripción de las inicializaciones\n",
    "### Inicialización del diccionario\n",
    "Para el diccionario se implemenraron dos estrategias. \n",
    "La inicialización \"1\" es inicializar el diccionario D con valores aleatorios. Es decir, cáda átomo es un vector de valores aleatorios y norma unitaria.\n",
    "La otra estrategia (o estrategia \"0\") es inicializar el diccionario con elementos aleatorios que pertenecen al dataset. De esta forma, cada átomo inicial del diccionario es exactamente un elemento del conjunto de datos.\n",
    "\n",
    "### Inicialización de la matriz de acumulación A\n",
    "La configuración \"1\" de inicialización de la matriz de acumulación A consta de inicializar esta matriz con valores aleatorios y que cada columna tenga norma unitaria. La configuración \"2\" es inicializar esta matriz con el valor constante 0.001. Mientras que la otra configuración, la inicialización \"0\" es inicializar esta matriz como una matriz estrictamente diagonal con valor 0.001 en su diagonal.\n",
    "\n",
    "### Inicialización de la matriz de acumulación B\n",
    "Para inicializar esta matriz hay implementadas 2 alternativas. La primer configuración para inicializar esta matriz, la configuración \"1\", es, al igual que en los dos casos anteriores, cargar la matriz con valores aleatorios y que su columnas tengan norma unitaria. La configuración \"2\" consisten en cargala con el valor constante 0.001, mientras que la otra configuración, la configuración \"0\", es cargar la matriz nula.\n",
    "\n",
    "## Procesamiento en Batches\n",
    "De acuerdo al paper en cuestión, se implementó la posibilidad de realizar procesamiento por batches y no únicamente por un único elemento por iteración. De esta forma se logró reducir drásticamente la cantidad de iteraciones del algoritmo que se requieren. Esto repercute en una reducción del tiempo de entrenamiento.\n",
    "Utilizando el procesamiento por batches, se realizaron una buena cantidad de pruebas que permitió observar como afecta a la eficacia del algoritmo las distintas \n",
    "\n",
    "## Descripción de la Loss\n",
    "### Loss Offline\n",
    "La loss que se presenta está calculada sobre un conjunto de test que está por fuera del conjunto. Para el calculo se toma promedio del error de reconstruir cada vector del conjunto de test.\n",
    "\n",
    "Esta Loss no se toma en cuenta para tomar acciones durante el entrenamiento como podría ser realizar un early stopping si se detecta que la Loss deja de decrecer de forma significativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados\n",
    "\n",
    "## Tabla comparativa de tiempos\n",
    "| Configuración| Init D | Init A | Init B | Optimizador | Lambda | Tamaño del batch | Cantidad de elementos | Tiempo/It | Tiempo/dato |\n",
    "|:---------|:------:|:------:|:------:|:-----------:|:------:|:----------------:|:---------------------:|:---------:|------------:| \n",
    "| 1 - base |    1   |    1   |   0    | Lasso-LARS  | 0.001  |        1         |          40000        |   104ms   |  104ms      |\n",
    "| 2        |    0   |    1   |   0    | Lasso-LARS  | 0.001  |        1         |          40000        |   289ms   |  289ms      |\n",
    "| 3        |    1   |    1   |   0    | Lasso-LARS  | 0.001  |        200       |          40000        |   1140ms  |  6ms        |\n",
    "| 4        |    1   |    1   |   0    | Lasso       | 0.001  |        200       |          40000        |   915ms   |  5ms        |\n",
    "| 5        |    0   |    1   |   0    | Lasso-LARS  | 0.01   |        200       |          40000        |   790ms   |  4ms        |\n",
    "| 6        |    1   |    1   |   0    | Lasso-LARS  | 0.01   |        200       |          40000        |   705ms   |  3.5ms      |\n",
    "| 7 k=1000 |    1   |    1   |   0    | Lasso-LARS  | 0.001  |        200       |          40000        |   5330ms  |  27ms     | \n",
    "\n",
    "\n",
    "## Comparación visual del proceso y graficas de losses.\n",
    "### Proceso de aprendizaje con batches unitarios - 40000 iteraciones\n",
    "![Gif 1](grilla-40k.gif)\n",
    "\n",
    "### Proceso de aprendizaje con batches de 200 elementos - 200 iteraciones\n",
    "![Gif 1](grilla-200.gif)\n",
    "\n",
    "### Comapración de frames iniciales, finales y losses\n",
    "|Desc| Configuración 1 - base | Configuración 2 |Configuración 3 |Configuración 4 |Configuración 5 |Configuración 6 |\n",
    "|--------------------------------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|\n",
    "|Tiempo dato| 104ms| 289ms| 6ms| 5ms| 4ms| 3.5ms|\n",
    "|Frame 1| ![Imagen 1](dict-250_its-40000_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-1/temp_frame_0.png) | ![Imagen 2](dict-250_its-40000_a-1_b-0_d-0_opt-lars_lamda-0.001_tbs-1/temp_frame_0.png) |![Imagen 3](dict-250_its-200_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-200/temp_frame_0.png) |![Imagen 4](dict-250_its-200_a-1_b-0_d-1_opt-lasso_lamda-0.001_tbs-200/temp_frame_0.png) |![Imagen 5](dict-250_its-200_a-1_b-0_d-1_opt-lars_lamda-0.01_tbs-200/temp_frame_0.png) |![Imagen 6](dict-250_its-200_a-1_b-0_d-0_opt-lars_lamda-0.01_tbs-200/temp_frame_0.png) |\n",
    "|Frame final| ![Imagen 1-1](dict-250_its-40000_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-1/temp_frame_final.png) | ![Imagen 2](dict-250_its-40000_a-1_b-0_d-0_opt-lars_lamda-0.001_tbs-1/temp_frame_final.png) |![Imagen 3](dict-250_its-200_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-200/temp_frame_final.png) |![Imagen 4](dict-250_its-200_a-1_b-0_d-1_opt-lasso_lamda-0.001_tbs-200/temp_frame_final.png) |![Imagen 5](dict-250_its-200_a-1_b-0_d-1_opt-lars_lamda-0.01_tbs-200/temp_frame_final.png) |![Imagen 6](dict-250_its-200_a-1_b-0_d-0_opt-lars_lamda-0.01_tbs-200/temp_frame_final.png)|\n",
    "|Loss| ![Imagen 1-1](dict-250_its-40000_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-1/OfflineLoss_por_iteracion.png) | ![Imagen 2](dict-250_its-40000_a-1_b-0_d-0_opt-lars_lamda-0.001_tbs-1/OfflineLoss_por_iteracion.png) |![Imagen 3](dict-250_its-200_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-200/OfflineLoss_por_iteracion.png) |![Imagen 4](dict-250_its-200_a-1_b-0_d-1_opt-lasso_lamda-0.001_tbs-200/OfflineLoss_por_iteracion.png) |![Imagen 5](dict-250_its-200_a-1_b-0_d-1_opt-lars_lamda-0.01_tbs-200/OfflineLoss_por_iteracion.png) |![Imagen 6](dict-250_its-200_a-1_b-0_d-0_opt-lars_lamda-0.01_tbs-200/OfflineLoss_por_iteracion.png)|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Bonus Track - Configuración 7 entrenamiento de un diccionario de 1000 elementos\n",
    "\n",
    "Diccionario de 1000 elementos obtenido con 200 iteraciones y 200 datos por batch.\n",
    "En este diccionario se puede observar que aún muchos de sus átomos son ruido. Esto indica que es necesario un entrenamiento más prolongado. A la vez que se observa una buena cantidad de caracteres nítidos.\n",
    "\n",
    "![Imagen 7](dict-1000_its-200_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-200/temp_frame_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "\n",
    "Se logró implementar un algoritmo para el aprendizaje Online de diccionarios y se logró implementar distintas mejoras que se proponen en la misma publicación.\n",
    "La implementación permitió comprobar de manera empírica los efectos de introducir las mejoras mencionadas en el documento. Si bien hay desarrollos que quedaron como trabajos futuros, la implementación lograda es un buen punto inicial que permite el desarrollo posterior para implementar una nueva tanda de mejoras.\n",
    "\n",
    "La implementación de algoritmo propuesto, así como las mejoras implementadas, fueron desafiantes y que requirieron de agudeza en el desarrollo.\n",
    "\n",
    "## Análisis de las pruebas realizadas\n",
    "Las pruebas realizadas dejan de manifiesto que utilizar Lasso como optimizador es más rápido en cada iteración que utilizar Lasso-LARS, sin embargo el resultado final luego de la misma cantidad de iteraciones es superior en el caso de Lasso-LARS.\n",
    "Se aprecia que la correcta elección del parámetro de regularización es relevante para que el algoritmo logre aprender, como se observa en los casos 5 y 6, donde al utilizar un valor de $\\lambda$ grande no se logra un aprendizaje significativo luego de haber utilizado 40000 datos.Cantidad de datos con la cual sí se observa un buen aprendizaje si se utiliza $\\lambda = 0.001$ como ocurre en los casos 1,2,3,4 y 7.\n",
    "A su vez, los casos 5 y 6 dejan de manifiesto que inicializar el diccionario con valores de los datos favorece el aprendizaje, pues en el caso 5 no se observa aprendizaje pero si se observa una pequeño aprendizaje en el caso 6. Esto se observa tanto al observar los frames iniciales y finales, así como observando las gráficas de la loss.\n",
    "\n",
    "Sin embargo la implementación de mejora que se lleva todas las miradas es el procesamiento en batches. La implementanción de esta mejora fue ardua, más su resultado permitió la realización de una mayor cantidad de experimentos e incluso realizar pruebas con diccionarios de mayor tamaño. En cuanto a esta mejora es notorio la reducción de tiempo en el procesamiento de cada dato individual, pasando de un promedio de 104ms o 289ms a estar en el orden 4ms, es decir que el procesamiento por batches es entre  26 y 76 veces más rápido para procesar cada muestra.\n",
    "\n",
    "Los resultados mostrados no permiten aportan información sobre el efecto de las distintas inicializaciones implementadas para las matrices A y B.\n",
    "\n",
    "Observando las losses y la visualización temporal de los resultados obtenidos, se aprecia que los algoritmos aún tienen capacidad de mejorar los resultados con mayor entrenamientos más largos y con otras mejoras como puede ser el purgado del diccionario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo futuro \n",
    "### Purgado del diccionario\n",
    "El paper propones utilizar una técnica de purgado de forma de detectar atomos que no están siendo utilizados y reinicializarlos, de forma de forzar a que el diccionario aprendido tenga átomos de mejor calidad y más representativos.\n",
    "\n",
    "### Early Stoping\n",
    "Esta propuesta no se encuentra concretamente en el paper, pero podría ser bueno implementar una versión de Early Stopping que permita detener el entrenamiento si las actualizaciones del diccionario son pequeñas o no hay mejoras en la loss, de forma de no perder tiempo computacional si el algoritmo ya no está mejorando su aprendizaje, e incluso permitir aventurarse a buscar diccionarios con una gran cantidad de iteraciones.\n",
    "\n",
    "### Warm Start\n",
    "Evaluar utilizar el algoritmo Lasso con Warm Start y evaluar la mejora de velocidad en la convergencia del algoritmo.\n",
    "\n",
    "### Impaintig\n",
    "Se desea implementar un algoritmo que permita realizar impainting para mejorar la reconstrucción de los caracteres del dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del código -  Clase de python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineDictionaryLearning:\n",
    "    \"\"\"\n",
    "    Aprendizaje en línea de diccionarios para codificación sparsa.\n",
    "\n",
    "    Esta clase implementa un algoritmo online para aprender un diccionario\n",
    "    que pueda representar de forma sparsa un conjunto de datos.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: np.array,\n",
    "        log_step: int = 40,\n",
    "        test_batch_size: int = 1000,\n",
    "        base_dir: str = \".\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inicializa la clase OnlineDictionaryLearning.\n",
    "\n",
    "        Args:\n",
    "            data (np.array): Conjunto de datos para el aprendizaje del diccionario.\n",
    "            log_step (int, optional): Cada cuantas iteraciones se registran los resultados. Por defecto 40.\n",
    "            test_batch_size (int, optional): Tamaño del batch de prueba. Por defecto 1000.\n",
    "            base_dir (str, optional): Directorio base para guardar registros y salidas. Por defecto \".\".\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_gen = self.sample(data)\n",
    "        self.n_obs = len(data)\n",
    "        self.dim_obs = len(data[0])\n",
    "        self.m = data.shape[1]\n",
    "\n",
    "        self.log_step = log_step\n",
    "        self.test_batch_size = test_batch_size\n",
    "\n",
    "        self.base_dir = base_dir\n",
    "        self.losses = []\n",
    "        self.offline_loss = []\n",
    "        self.objective = []\n",
    "        self.cumulative_losses = []\n",
    "        self.imagenes = []\n",
    "        self.test_batch = iter(())\n",
    "        np.random.seed(14)  # semilla para hacer pruebas comparables\n",
    "        if not os.path.exists(base_dir):\n",
    "            os.makedirs(base_dir)\n",
    "\n",
    "    def sample(self, data: np.array):\n",
    "        \"\"\"\n",
    "        Crea un generador aleatorio de muestras de datos sobre el cual iterar.\n",
    "\n",
    "        Args:\n",
    "            data (np.array): Conjunto de datos para muestreo.\n",
    "\n",
    "        Yields:\n",
    "            np.array: Una muestra aleatoria del conjunto de datos.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            permutation = list(np.random.permutation(self.n_obs))\n",
    "            for idx in permutation:\n",
    "                yield data[idx]\n",
    "\n",
    "    def initialize_logs(self):\n",
    "        \"\"\"\n",
    "        Inicializa las listas para registrar Loss, imágenes y obtiene un generado de datos de prueba.\n",
    "        \"\"\"\n",
    "        self.losses = []\n",
    "        self.offline_loss = []\n",
    "        self.imagenes = []\n",
    "        self.test_batch = iter(\n",
    "            [next(self.data_gen) for i in range(self.test_batch_size)]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_alpha(x: np.array, dic: np.array, lam, optimizer: str = \"lasso\"):\n",
    "        \"\"\"\n",
    "        Calcula los coeficientes de representación esparsa para una observación dada.\n",
    "\n",
    "        Args:\n",
    "            x (np.array): Vector de observación.\n",
    "            dic (np.array): Matriz del diccionario.\n",
    "            lam (float): Parámetro de regularización.\n",
    "            optimizer (str, optional): Método de optimización ('lasso' o 'lars'). Por defecto \"lasso\".\n",
    "\n",
    "        Returns:\n",
    "            np.array: Coeficientes de representación esparsa.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Si NO se especifica un optimizador inválido.\n",
    "        \"\"\"\n",
    "\n",
    "        if optimizer == \"lasso\":\n",
    "            reg = Lasso(alpha=lam)\n",
    "        elif optimizer == \"lars\":\n",
    "            reg = LassoLars(alpha=lam)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Optimizador incorrecto, solo se aceptan 'lasso' (dafault) o 'lars' \"\n",
    "            )\n",
    "        reg.fit(X=dic, y=x)\n",
    "        return reg.coef_\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_dic(A: np.array, B: np.array, D: np.array, k: int):\n",
    "        \"\"\"\n",
    "        Actualiza el diccionario utilizando las matrices acumuladas A y B.\n",
    "\n",
    "        Args:\n",
    "            A (np.array): Matriz de coeficientes acumulada.\n",
    "            B (np.array): Matriz acumulada de productos de datos.\n",
    "            D (np.array): Diccionario actual.\n",
    "            k (int): Número de átomos en el diccionario.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Diccionario actualizado.\n",
    "        \"\"\"\n",
    "        tolerancia = 1.0e-7\n",
    "        error = 1\n",
    "        o = 0\n",
    "        D_nuevo = D.copy()\n",
    "        # while not converged :\n",
    "        while error > tolerancia and o < 10:\n",
    "            for j in range(k):\n",
    "                u_j = (B[:, j] - np.matmul(D, A[:, j])) / A[j, j] + D[:, j]\n",
    "                D_nuevo[:, j] = u_j / max(np.linalg.norm(u_j), 1)\n",
    "            D = D_nuevo / np.linalg.norm(D_nuevo, axis=0)\n",
    "            error = np.linalg.norm(D_nuevo - D, ord=\"fro\")\n",
    "            o = o + 1\n",
    "        return D\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_A_B(\n",
    "        A_prev: np.array,\n",
    "        B_prev: np.array,\n",
    "        x_i_batch: np.array,\n",
    "        alphas: np.array,\n",
    "        beta: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Actualiza las matrices A y B utilizadas en la optimización del diccionario.\n",
    "\n",
    "        Args:\n",
    "            A_prev (np.array): Matriz A acumulada previa.\n",
    "            B_prev (np.array): Matriz B acumulada previa.\n",
    "            x_i (np.array): Observación actual.\n",
    "            alpha_i (np.array): Coeficientes esparsos  para la observación.\n",
    "            beta (int, optional): Parámetro de ponderación. Por defecto 1.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Matrices A y B actualizadas.\n",
    "        \"\"\"\n",
    "\n",
    "        A_curr = beta * A_prev + sum(\n",
    "            [np.outer(alpha_i, alpha_i.T) for alpha_i in alphas]\n",
    "        )\n",
    "        B_curr = beta * B_prev + sum(\n",
    "            [np.outer(x_i, alpha_i.T) for x_i, alpha_i in zip(x_i_batch, alphas)]\n",
    "        )\n",
    "        A_prev = A_curr\n",
    "        B_prev = B_curr\n",
    "        return A_curr, B_curr\n",
    "\n",
    "    def learn(\n",
    "        self,\n",
    "        it: int,\n",
    "        lam: float,\n",
    "        k: int,\n",
    "        train_batch_size: int = 1,\n",
    "        optimizer: str = \"lasso\",\n",
    "        init_A_mod: int = 1,\n",
    "        init_B_mod: int = 1,\n",
    "        init_D_mod: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ejecuta el aprendizaje del diccionario.\n",
    "\n",
    "        Args:\n",
    "            it (int): Cantidad de iteraciones.\n",
    "            lam (float): Parámetro de regularización.\n",
    "            k (int): Número de átomos en el diccionario.\n",
    "            train_batch_size (int, optional): Tamaño del batch de entrenamiento. Por defecto 1.\n",
    "            optimizer (str, optional): Método de optimización ('lasso' o 'lars'). Por defecto \"lasso\".\n",
    "            init_A_mod (int, optional): Método de inicialización para la matriz A. Por defecto 1.\n",
    "            init_B_mod (int, optional): Método de inicialización para la matriz B. Por defecto 1.\n",
    "            init_D_mod (int, optional): Método de inicialización para el diccionario D. Por defecto 1.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Diccionario aprendido.\n",
    "        \"\"\"\n",
    "        assert train_batch_size >= 1, \"train_batch_size tiene que ser >= 1\"\n",
    "        self.initialize_logs()\n",
    "\n",
    "        # Init A\n",
    "        if init_A_mod == 1:\n",
    "            A_prev = np.random.randn(k, k)\n",
    "            A_prev /= np.linalg.norm(A_prev)\n",
    "        elif init_A_mod == 2:\n",
    "            A_prev = 0.001 * np.ones((k, k))\n",
    "        else:\n",
    "            A_prev = 0.001 * np.identity(k)\n",
    "\n",
    "        # Init B\n",
    "        if init_B_mod == 1:\n",
    "            B_prev = 0.001 * np.random.randn(self.m, k)\n",
    "        if init_B_mod == 2:\n",
    "            B_prev = 0.001 * np.ones((self.m, k))\n",
    "        else:\n",
    "            B_prev = np.zeros((self.m, k))\n",
    "\n",
    "        # Init D\n",
    "        D_prev = self.initialize_dic(k, self.m, self.data_gen, init_D_mod)\n",
    "\n",
    "        for it_curr in tqdm(range(it)):\n",
    "            x_i_batch = [next(self.data_gen) for i in range(train_batch_size)]\n",
    "            alphas = [\n",
    "                self.compute_alpha(x_i, D_prev, lam, optimizer=optimizer)\n",
    "                for x_i in x_i_batch\n",
    "            ]\n",
    "            if train_batch_size == 1:\n",
    "                beta = 1\n",
    "            elif train_batch_size > 1:\n",
    "                if (it_curr + 1) + 1 < train_batch_size:\n",
    "                    theta = (it_curr + 1) * train_batch_size\n",
    "                else:\n",
    "                    theta = train_batch_size**2 + (it_curr + 1) - train_batch_size\n",
    "                beta = (theta + 1 - train_batch_size) / (theta + 1)\n",
    "            A_curr, B_curr = self.compute_A_B(\n",
    "                A_prev, B_prev, x_i_batch, alphas, beta=beta\n",
    "            )\n",
    "            D_curr = self.compute_dic(A=A_curr, B=B_curr, D=D_prev, k=k)\n",
    "            A_prev = A_curr\n",
    "            B_prev = B_curr\n",
    "            D_prev = D_curr\n",
    "\n",
    "            if it_curr % self.log_step == 0:\n",
    "                self.log(\n",
    "                    observation=x_i_batch[0],\n",
    "                    dictionary=D_curr,\n",
    "                    it=it_curr,\n",
    "                    lam=lam,\n",
    "                    alpha=alphas[0],\n",
    "                )\n",
    "\n",
    "        self.compute_objective()\n",
    "\n",
    "        mosaic = util.mosaico(np.array(D_curr.T))\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(mosaic, cmap=\"gray\")\n",
    "        plt.title(f\"Atomos \\n Final\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            f\"{self.base_dir}{os.sep}temp_frame_final.png\"\n",
    "        )  # Guarda la imagen temporalmente\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        self.imagenes.append(\n",
    "            f\"{self.base_dir}{os.sep}temp_frame_final.png\"\n",
    "        )  # Agrega el nombre a la lista\n",
    "\n",
    "        x = np.arange(0, len(self.losses), max(len(self.losses) // 10, 1))\n",
    "        xticks_labels = x * self.log_step\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.plot(self.cumulative_losses)\n",
    "        plt.title(f\"Loss acumulada\")\n",
    "        plt.xticks(x, xticks_labels)\n",
    "        plt.xlabel(\"Iteración\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            f\"{self.base_dir}{os.sep}Loss_acumulada.png\"\n",
    "        )  # Guarda la imagen temporalmente\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.plot(self.losses)\n",
    "        plt.xticks(x, xticks_labels)\n",
    "        plt.title(f\"Loss en la iteración\")\n",
    "        plt.xlabel(\"Iteración\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            f\"{self.base_dir}{os.sep}Loss_por_iteracion.png\"\n",
    "        )  # Guarda la imagen temporalmente\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.plot(self.offline_loss)\n",
    "        plt.xticks(x, xticks_labels)\n",
    "        plt.title(f\"Offline loss en la iteración\")\n",
    "        plt.xlabel(\"Iteración\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            f\"{self.base_dir}{os.sep}OfflineLoss_por_iteracion.png\"\n",
    "        )  # Guarda la imagen temporalmente\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        with imageio.get_writer(\n",
    "            f\"{self.base_dir}{os.sep}proceso_diccionario_its-{it}_lam-{lam}_k-{k}_{optimizer}_A-{init_A_mod}_B-{init_B_mod}_D-{init_D_mod}.gif\",\n",
    "            mode=\"I\",\n",
    "            duration=0.5,\n",
    "        ) as writer:\n",
    "            for imagen in self.imagenes:\n",
    "                frame = imageio.imread(imagen)\n",
    "                writer.append_data(frame)\n",
    "        return D_curr.T\n",
    "\n",
    "    def log(\n",
    "        self,\n",
    "        observation: np.array,\n",
    "        dictionary: np.array,\n",
    "        it: int,\n",
    "        lam: float,\n",
    "        alpha: np.array,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Registra el estado actual, incluyendo losss e imágenes.\n",
    "\n",
    "        Args:\n",
    "            observation (np.array): Observación actual.\n",
    "            dictionary (np.array): Diccionario actual.\n",
    "            it (int): Iteración actual.\n",
    "            lam (float): Parámetro de regularización.\n",
    "            alpha (np.array): Coeficientes sparsos  de la observación.\n",
    "        \"\"\"\n",
    "        loss = self.one_loss(observation, dictionary, alpha)\n",
    "        self.losses.append(loss)\n",
    "        self.cumulative_losses.append(self.cumulative_loss())\n",
    "        self.offline_loss.append(self.full_dataset_loss(dictionary, lam))\n",
    "        image_path = f\"{self.base_dir}{os.sep}temp_frame_{it}.png\"\n",
    "        mosaic = util.mosaico(np.array(dictionary.T))\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(mosaic, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Atomos \\n Iteración {it}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(image_path)  # Guarda la imagen temporalmente\n",
    "        plt.close()\n",
    "        self.imagenes.append(image_path)  # Agrega el nombre a la lista\n",
    "\n",
    "    def cumulative_loss(self):\n",
    "        \"\"\"\n",
    "        Calcula la loss acumulativa para las muestras observadas.\n",
    "\n",
    "        Args:\n",
    "            dictionary (np.array): Diccionario actual.\n",
    "\n",
    "        Returns:\n",
    "            float: Valor de loss acumulada.\n",
    "        \"\"\"\n",
    "        n_observed = len(self.losses)\n",
    "\n",
    "        return np.mean([self.losses[i] for i in range(n_observed)])\n",
    "\n",
    "    @staticmethod\n",
    "    def one_loss(x, dictionary: np.array, alpha: np.array):\n",
    "        \"\"\"\n",
    "        Calcula la loss de reconstrucción para una observación.\n",
    "\n",
    "        Args:\n",
    "            x (np.array): Observación.\n",
    "            dictionary (np.array): Diccionario.\n",
    "            alpha (np.array): Coeficientes dispersos.\n",
    "\n",
    "        Returns:\n",
    "            float: loss de reconstrucción.\n",
    "        \"\"\"\n",
    "        return np.linalg.norm(x - np.matmul(dictionary, alpha), ord=2) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_dic(k: int, m: int, data_gen, init_D_mod: int = 0):\n",
    "        \"\"\"\n",
    "        Inicializa la matriz del diccionario.\n",
    "\n",
    "        Args:\n",
    "            k (int): Número de átomos en el diccionario.\n",
    "            m (int): Dimensión de las observaciones.\n",
    "            data_gen (generator): Generador para muestreo de observaciones.\n",
    "            init_D_mod (int, optional): Método de inicialización. Por defecto 0.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Matriz del diccionario inicializada.\n",
    "        \"\"\"\n",
    "        if init_D_mod == 1:\n",
    "            D = np.random.randn(m, k)\n",
    "            return D / np.linalg.norm(D, axis=0)\n",
    "        else:\n",
    "            return np.array([next(data_gen) for _ in range(k)]).T\n",
    "\n",
    "    def observation_loss(self, x_i: np.array, dictionary: np.array, lam: float):\n",
    "        \"\"\"\n",
    "        Calcula la loss para una observación.\n",
    "\n",
    "        Args:\n",
    "            x_i (np.array): Observación.\n",
    "            dictionary (np.array): Diccionario.\n",
    "            lam (float): Parámetro de regularización.\n",
    "\n",
    "        Returns:\n",
    "            float: loss de la observación.\n",
    "        \"\"\"\n",
    "        alpha = self.compute_alpha(x_i, dictionary, lam)\n",
    "        return np.linalg.norm(x_i - np.matmul(dictionary, alpha), ord=2) ** 2\n",
    "\n",
    "    def full_dataset_loss(self, dictionary: np.array, lam: float):\n",
    "        \"\"\"\n",
    "        Calcula la loss total para el conjunto de datos de prueba.\n",
    "\n",
    "        Args:\n",
    "            dictionary (np.array): Diccionario actual.\n",
    "            lam (float): Parámetro de regularización.\n",
    "\n",
    "        Returns:\n",
    "            float: Pérdida total del conjunto de datos.\n",
    "        \"\"\"\n",
    "        self.test_batch, data_gen = tee(self.test_batch)\n",
    "        return np.mean(\n",
    "            [\n",
    "                self.observation_loss(next(data_gen), dictionary, lam)\n",
    "                for _ in range(self.test_batch_size)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def compute_objective(self):\n",
    "        \"\"\"\n",
    "        Calcula el valor de la función objetivo a lo largo de las iteraciones.\n",
    "\n",
    "        La función objetivo es la loss acumulativa promedio en cada iteración.\n",
    "        \"\"\"\n",
    "        cumulateD_loss = np.cumsum(self.losses)\n",
    "        self.objective = [\n",
    "            cumulateD_loss[i] / (i + 1) for i in range(len(cumulateD_loss))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos de procseamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar base de datos de caracteres LUISA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luisa = datos.get_char_luisa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set configuración general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "dict_size = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración 1 (base)\n",
    "Como configuración base se tomó la inicialización \"1\" para el diccionario D. Es decir con valores aleatorios, como se describió en la sección anterior.\n",
    "La matriz de acumulación A también está inicializada con valores aleatorios con norma 1 en las columnas, con la inicialización \"1\". Y la matriz B fue inicializada con la inicialización \"0\", con una matriz nula.\n",
    "En ésta configuración de base se utilizó un batch size de entrenamiento con un único elemento, y como optimizaro Lasso-Lars con parámetro de regularización $\\lambda=0.001$.\n",
    "\n",
    "### Gif del entrenamiento\n",
    "![1-gif.gif](dict-250_its-40000_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-1/proceso_diccionario_its-40000_lam-0.001_k-250_lars_A-1_B-0_D-1.gif)\n",
    "### Graficas Loss\n",
    "![1-loss-offline.png](dict-250_its-40000_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-1/OfflineLoss_por_iteracion.png)\n",
    "### Primer iter, 18k iter, 40k iter\n",
    "![1-frame1.png](dict-250_its-40000_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-1/temp_frame_0.png)\n",
    "![1-frame200.png](dict-250_its-40000_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-1/temp_frame_200.png)\n",
    "![1-frame40k.png](dict-250_its-40000_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-1/temp_frame_final.png)\n",
    "### Tiempo de ejecución\n",
    "Este entrenamiento se completó en 1h 09m 33s. Es decir que consumió 104ms en cada iteración.\n",
    "\n",
    "### Cantidad de datos utilizados\n",
    "En este entrenamiento se utilizaron 40.000 datos en batches de un único elemento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo para entrenar el diccionario Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"a\": 1,\n",
    "    \"b\": 0,\n",
    "    \"d\": 1,\n",
    "    \"opt\": \"lars\",\n",
    "    \"lamd\": 0.001,\n",
    "    \"train_b_s\": 1,\n",
    "    \"iteraciones\": 40000,\n",
    "    \"dict_size\": 250,\n",
    "}\n",
    "base_dir = f\"dict-{conf['dict_size']}_its-{conf['iteraciones']}_a-{conf['a']}_b-{conf['b']}_d-{conf['d']}_opt-{conf['opt']}_lamda-{conf['lamd']}_tbs-{conf['train_b_s']}\"\n",
    "ODL = OnlineDictionaryLearning(\n",
    "    luisa,\n",
    "    test_batch_size=1000,\n",
    "    base_dir=base_dir,\n",
    ")\n",
    "D = ODL.learn(\n",
    "    it=conf[\"iteraciones\"],\n",
    "    lam=conf[\"lamd\"],\n",
    "    k=conf[\"dict_size\"],\n",
    "    optimizer=conf[\"opt\"],\n",
    "    init_A_mod=conf[\"a\"],\n",
    "    init_B_mod=conf[\"b\"],\n",
    "    init_D_mod=conf[\"d\"],\n",
    "    train_batch_size=conf[\"train_b_s\"],\n",
    ")\n",
    "gif_name = f\"proceso_diccionario_its-{conf['iteraciones']}_lam-{conf['lamd']}_k-{conf['dict_size']}_{conf['opt']}_A-{conf['a']}_B-{conf['b']}_D-{conf['d']}.gif\"\n",
    "paths.append(f\"{base_dir}/{gif_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración 2\n",
    "La diferencia con el caso base está en la inicialización de los átomos del diccionario.  En esta configuración fue inicializado con la inicialización \"0\", Es que cada átomo fue inicializado con un elemento al azar del dataset.\n",
    "\n",
    "### Gif del entrenamiento\n",
    "![2-gif.gif](dict-250_its-40000_a-1_b-0_d-0_opt-lars_lamda-0.001_tbs-1/proceso_diccionario_its-40000_lam-0.001_k-250_lars_A-1_B-0_D-0.gif)\n",
    "### Graficas Loss\n",
    "![2-loss-offline.png](dict-250_its-40000_a-1_b-0_d-0_opt-lars_lamda-0.001_tbs-1/OfflineLoss_por_iteracion.png)\n",
    "### Primer iter, 18k iter, 40k iter\n",
    "![2-frame1.png](dict-250_its-40000_a-1_b-0_d-0_opt-lars_lamda-0.001_tbs-1/temp_frame_0.png)\n",
    "![2-frame5k.png](dict-250_its-40000_a-1_b-0_d-0_opt-lars_lamda-0.001_tbs-1/temp_frame_5000.png)\n",
    "![2-frame18k.png](dict-250_its-40000_a-1_b-0_d-0_opt-lars_lamda-0.001_tbs-1/temp_frame_18000.png)\n",
    "![2-frame40k.png](dict-250_its-40000_a-1_b-0_d-0_opt-lars_lamda-0.001_tbs-1/temp_frame_final.png)\n",
    "### Tiempo de ejecución\n",
    "Este entrenamiento se completó en 3h 12m 44s. Es decir que consumió 289ms en cada iteración.\n",
    "\n",
    "### Cantidad de datos utilizados\n",
    "En este entrenamiento se utilizaron 40.000 datos en batches de un único elemento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo para entrenar algoritmo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"a\": 1,\n",
    "    \"b\": 0,\n",
    "    \"d\": 0,\n",
    "    \"opt\": \"lars\",\n",
    "    \"lamd\": 0.001,\n",
    "    \"train_b_s\": 1,\n",
    "    \"iteraciones\": 40000,\n",
    "    \"dict_size\": 250,\n",
    "}\n",
    "base_dir = f\"dict-{conf['dict_size']}_its-{conf['iteraciones']}_a-{conf['a']}_b-{conf['b']}_d-{conf['d']}_opt-{conf['opt']}_lamda-{conf['lamd']}_tbs-{conf['train_b_s']}\"\n",
    "ODL = OnlineDictionaryLearning(\n",
    "    luisa,\n",
    "    test_batch_size=1000,\n",
    "    base_dir=base_dir,\n",
    "    log_step=conf[\"log\"],\n",
    ")\n",
    "D = ODL.learn(\n",
    "    it=conf[\"iteraciones\"],\n",
    "    lam=conf[\"lamd\"],\n",
    "    k=conf[\"dict_size\"],\n",
    "    optimizer=conf[\"opt\"],\n",
    "    init_A_mod=conf[\"a\"],\n",
    "    init_B_mod=conf[\"b\"],\n",
    "    init_D_mod=conf[\"d\"],\n",
    "    train_batch_size=conf[\"train_b_s\"],\n",
    ")\n",
    "gif_name = f\"proceso_diccionario_its-{conf['iteraciones']}_lam-{conf['lamd']}_k-{conf['dict_size']}_{conf['opt']}_A-{conf['a']}_B-{conf['b']}_D-{conf['d']}.gif\"\n",
    "paths.append(f\"{base_dir}/{gif_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración 3\n",
    "La diferencia con el caso base está en el tamaño del batch y la cantidad de iteraciones.\n",
    "En este caso, se utilizaron 200 iteraciones y 200 muestras en batch.El log se realiza cada 10 iteraciones.\n",
    "En las siguientes configuraciones también se utilizará esta cantidad de iteraciones y el tamaño del batch que se utilizará.\n",
    "\n",
    "### Gif del entrenamiento\n",
    "![3-gif.gif](dict-250_its-200_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-200/proceso_diccionario_its-200_lam-0.001_k-250_lars_A-1_B-0_D-1.gif)\n",
    "### Graficas Loss\n",
    "![3-loss-offline.png](dict-250_its-200_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-200/OfflineLoss_por_iteracion.png)\n",
    "### Primer iter, 18k iter, 40k iter\n",
    "![3-frame1.png](dict-250_its-200_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-200/temp_frame_0.png)\n",
    "![3-framefinal.png](dict-250_its-200_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-200/temp_frame_final.png)\n",
    "### Tiempo de ejecución\n",
    "Este entrenamiento se completó en 3m 48s. Es decir que consumió en promedio 6ms en procesar cada dato.\n",
    "\n",
    "### Cantidad de datos utilizados\n",
    "En este entrenamiento se utilizaron 40.000 datos en batches de 200 elementos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de entrenamiento de la configuración 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"a\": 1,\n",
    "    \"b\": 0,\n",
    "    \"d\": 1,\n",
    "    \"opt\": \"lars\",\n",
    "    \"lamd\": 0.001,\n",
    "    \"train_b_s\": 200,\n",
    "    \"iteraciones\": 200,\n",
    "    \"log\": 10,\n",
    "    \"dict_size\": 250,\n",
    "}\n",
    "\n",
    "base_dir = f\"dict-{conf['dict_size']}_its-{conf['iteraciones']}_a-{conf['a']}_b-{conf['b']}_d-{conf['d']}_opt-{conf['opt']}_lamda-{conf['lamd']}_tbs-{conf['train_b_s']}\"\n",
    "ODL = OnlineDictionaryLearning(\n",
    "    luisa,\n",
    "    test_batch_size=1000,\n",
    "    base_dir=base_dir,\n",
    "    log_step=conf[\"log\"],\n",
    ")\n",
    "D = ODL.learn(\n",
    "    it=conf[\"iteraciones\"],\n",
    "    lam=conf[\"lamd\"],\n",
    "    k=conf[\"dict_size\"],\n",
    "    optimizer=conf[\"opt\"],\n",
    "    init_A_mod=conf[\"a\"],\n",
    "    init_B_mod=conf[\"b\"],\n",
    "    init_D_mod=conf[\"d\"],\n",
    "    train_batch_size=conf[\"train_b_s\"],\n",
    ")\n",
    "gif_name = f\"proceso_diccionario_its-{conf['iteraciones']}_lam-{conf['lamd']}_k-{conf['dict_size']}_{conf['opt']}_A-{conf['a']}_B-{conf['b']}_D-{conf['d']}.gif\"\n",
    "paths.append(f\"{base_dir}/{gif_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración 4\n",
    "Es el algoritmo de optimización. Se utiliza Lasso en lugar de Lasso-LARS.\n",
    "\n",
    "### Gif del entrenamiento\n",
    "![4-gif.gif](dict-250_its-200_a-1_b-0_d-1_opt-lasso_lamda-0.001_tbs-200/proceso_diccionario_its-200_lam-0.001_k-250_lasso_A-1_B-0_D-1.gif)\n",
    "### Graficas Loss\n",
    "![4-loss-offline.png](dict-250_its-200_a-1_b-0_d-1_opt-lasso_lamda-0.001_tbs-200/OfflineLoss_por_iteracion.png)\n",
    "### Primer iter, 18k iter, 40k iter\n",
    "![4-frame1.png](dict-250_its-200_a-1_b-0_d-1_opt-lasso_lamda-0.001_tbs-200/temp_frame_0.png)\n",
    "![4-frame40k.png](dict-250_its-200_a-1_b-0_d-1_opt-lasso_lamda-0.001_tbs-200/temp_frame_final.png)\n",
    "### Tiempo de ejecución\n",
    "Este entrenamiento se completó en 3m 03s. Es decir que consumió 5ms en procesar cada dato.\n",
    "\n",
    "### Cantidad de datos utilizados\n",
    "En este entrenamiento se utilizaron 40.000 datos en batches de 200 elementos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo para entrenar configuración 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    {\n",
    "        \"a\": 1,\n",
    "        \"b\": 0,\n",
    "        \"d\": 1,\n",
    "        \"opt\": \"lasso\",\n",
    "        \"lamd\": 0.001,\n",
    "        \"train_b_s\": 200,\n",
    "        \"iteraciones\": 200,\n",
    "        \"log\": 10,\n",
    "        \"dict_size\": 250,\n",
    "    },\n",
    ")\n",
    "\n",
    "base_dir = f\"dict-{conf['dict_size']}_its-{conf['iteraciones']}_a-{conf['a']}_b-{conf['b']}_d-{conf['d']}_opt-{conf['opt']}_lamda-{conf['lamd']}_tbs-{conf['train_b_s']}\"\n",
    "ODL = OnlineDictionaryLearning(\n",
    "    luisa,\n",
    "    test_batch_size=1000,\n",
    "    base_dir=base_dir,\n",
    "    log_step=conf[\"log\"],\n",
    ")\n",
    "D = ODL.learn(\n",
    "    it=conf[\"iteraciones\"],\n",
    "    lam=conf[\"lamd\"],\n",
    "    k=conf[\"dict_size\"],\n",
    "    optimizer=conf[\"opt\"],\n",
    "    init_A_mod=conf[\"a\"],\n",
    "    init_B_mod=conf[\"b\"],\n",
    "    init_D_mod=conf[\"d\"],\n",
    "    train_batch_size=conf[\"train_b_s\"],\n",
    ")\n",
    "gif_name = f\"proceso_diccionario_its-{conf['iteraciones']}_lam-{conf['lamd']}_k-{conf['dict_size']}_{conf['opt']}_A-{conf['a']}_B-{conf['b']}_D-{conf['d']}.gif\"\n",
    "paths.append(f\"{base_dir}/{gif_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración 5\n",
    "La diferencia con el caso base está en el parámetro de regularización. Se utiliza $\\lambda=0.01$\n",
    "\n",
    "### Gif del entrenamiento\n",
    "![5-gif.gif](dict-250_its-200_a-1_b-0_d-1_opt-lars_lamda-0.01_tbs-200/proceso_diccionario_its-200_lam-0.01_k-250_lars_A-1_B-0_D-1.gif)\n",
    "### Graficas Loss\n",
    "![5-loss-offline.png](dict-250_its-200_a-1_b-0_d-1_opt-lars_lamda-0.01_tbs-200/OfflineLoss_por_iteracion.png)\n",
    "### Primer iter, 18k iter, 40k iter\n",
    "![5-frame1.png](dict-250_its-200_a-1_b-0_d-1_opt-lars_lamda-0.01_tbs-200/temp_frame_0.png)\n",
    "![5-frame40k.png](dict-250_its-200_a-1_b-0_d-1_opt-lars_lamda-0.01_tbs-200/temp_frame_final.png)\n",
    "### Tiempo de ejecución\n",
    "Este entrenamiento se completó en 2m 38s. Es decir que consumió en promedio 4ms en procesar cada dato.\n",
    "\n",
    "### Cantidad de datos utilizados\n",
    "En este entrenamiento se utilizaron 40.000 datos en batches de 200 elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo para entrenar configuración 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"a\": 1,\n",
    "    \"b\": 0,\n",
    "    \"d\": 1,\n",
    "    \"opt\": \"lars\",\n",
    "    \"lamd\": 0.01,\n",
    "    \"train_b_s\": 200,\n",
    "    \"iteraciones\": 200,\n",
    "    \"log\": 10,\n",
    "    \"dict_size\": 250,\n",
    "}\n",
    "\n",
    "base_dir = f\"dict-{conf['dict_size']}_its-{conf['iteraciones']}_a-{conf['a']}_b-{conf['b']}_d-{conf['d']}_opt-{conf['opt']}_lamda-{conf['lamd']}_tbs-{conf['train_b_s']}\"\n",
    "ODL = OnlineDictionaryLearning(\n",
    "    luisa,\n",
    "    test_batch_size=1000,\n",
    "    base_dir=base_dir,\n",
    "    log_step=conf[\"log\"],\n",
    ")\n",
    "D = ODL.learn(\n",
    "    it=conf[\"iteraciones\"],\n",
    "    lam=conf[\"lamd\"],\n",
    "    k=conf[\"dict_size\"],\n",
    "    optimizer=conf[\"opt\"],\n",
    "    init_A_mod=conf[\"a\"],\n",
    "    init_B_mod=conf[\"b\"],\n",
    "    init_D_mod=conf[\"d\"],\n",
    "    train_batch_size=conf[\"train_b_s\"],\n",
    ")\n",
    "gif_name = f\"proceso_diccionario_its-{conf['iteraciones']}_lam-{conf['lamd']}_k-{conf['dict_size']}_{conf['opt']}_A-{conf['a']}_B-{conf['b']}_D-{conf['d']}.gif\"\n",
    "paths.append(f\"{base_dir}/{gif_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración 6\n",
    "La diferencia con el caso base está en el parámetro de regularización y la inicialización del diccionario. Se utiliza $\\lambda=0.01$ e inicialización \"0\" del diccionario, es decir utilizando elementos del dataset como átomos.\n",
    "\n",
    "### Gif del entrenamiento\n",
    "![6-gif.gif](dict-250_its-200_a-1_b-0_d-0_opt-lars_lamda-0.01_tbs-200/proceso_diccionario_its-200_lam-0.01_k-250_lars_A-1_B-0_D-0.gif)\n",
    "### Graficas Loss\n",
    "![6-loss-offline.png](dict-250_its-200_a-1_b-0_d-0_opt-lars_lamda-0.01_tbs-200/OfflineLoss_por_iteracion.png)\n",
    "### Primer iter, 18k iter, 40k iter\n",
    "![6-frame1.png](dict-250_its-200_a-1_b-0_d-0_opt-lars_lamda-0.01_tbs-200/temp_frame_0.png)\n",
    "![6-frame40k.png](dict-250_its-200_a-1_b-0_d-0_opt-lars_lamda-0.01_tbs-200/temp_frame_final.png)\n",
    "### Tiempo de ejecución\n",
    "Este entrenamiento se completó en 3m 03s. Es decir que consumió en promedio 3.5ms en procesar cada dato.\n",
    "\n",
    "### Cantidad de datos utilizados\n",
    "En este entrenamiento se utilizaron 40.000 datos en batches de 200 elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo para entrenar configuración 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    {\n",
    "        \"a\": 1,\n",
    "        \"b\": 0,\n",
    "        \"d\": 0,\n",
    "        \"opt\": \"lars\",\n",
    "        \"lamd\": 0.01,\n",
    "        \"train_b_s\": 200,\n",
    "        \"iteraciones\": 200,\n",
    "        \"log\": 10,\n",
    "        \"dict_size\": 250,\n",
    "    },\n",
    ")\n",
    "\n",
    "base_dir = f\"dict-{conf['dict_size']}_its-{conf['iteraciones']}_a-{conf['a']}_b-{conf['b']}_d-{conf['d']}_opt-{conf['opt']}_lamda-{conf['lamd']}_tbs-{conf['train_b_s']}\"\n",
    "ODL = OnlineDictionaryLearning(\n",
    "    luisa,\n",
    "    test_batch_size=1000,\n",
    "    base_dir=base_dir,\n",
    "    log_step=conf[\"log\"],\n",
    ")\n",
    "D = ODL.learn(\n",
    "    it=conf[\"iteraciones\"],\n",
    "    lam=conf[\"lamd\"],\n",
    "    k=conf[\"dict_size\"],\n",
    "    optimizer=conf[\"opt\"],\n",
    "    init_A_mod=conf[\"a\"],\n",
    "    init_B_mod=conf[\"b\"],\n",
    "    init_D_mod=conf[\"d\"],\n",
    "    train_batch_size=conf[\"train_b_s\"],\n",
    ")\n",
    "gif_name = f\"proceso_diccionario_its-{conf['iteraciones']}_lam-{conf['lamd']}_k-{conf['dict_size']}_{conf['opt']}_A-{conf['a']}_B-{conf['b']}_D-{conf['d']}.gif\"\n",
    "paths.append(f\"{base_dir}/{gif_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración 7\n",
    "En esta configuración se agranda el diccionario a 1000 átomos. \n",
    "La diferencia con la configuración de base es que se emplea la inicialización \"0\" del diccionario.\n",
    "\n",
    "\n",
    "### Gif del entrenamiento\n",
    "![7-gif.gif](dict-1000_its-200_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-200/proceso_diccionario_its-200_lam-0.001_k-1000_lars_A-1_B-0_D-1.gif)\n",
    "### Graficas Loss\n",
    "![7-loss-offline.png](dict-1000_its-200_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-200/OfflineLoss_por_iteracion.png)\n",
    "### Primer iter, 18k iter, 40k iter\n",
    "![7-frame1.png](dict-1000_its-200_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-200/temp_frame_0.png)\n",
    "![7-frame40k.png](dict-1000_its-200_a-1_b-0_d-1_opt-lars_lamda-0.001_tbs-200/temp_frame_final.png)\n",
    "### Tiempo de ejecución\n",
    "Este entrenamiento se completó en 17m 46s. Es decir que en promedio se consumió 27ms en procesar cada dato.\n",
    "\n",
    "### Cantidad de datos utilizados\n",
    "En este entrenamiento se utilizaron 40.000 datos en batches de 200 elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo para entrenar configuración 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    {\n",
    "        \"a\": 1,\n",
    "        \"b\": 0,\n",
    "        \"d\": 1,\n",
    "        \"opt\": \"lars\",\n",
    "        \"lamd\": 0.01,\n",
    "        \"train_b_s\": 200,\n",
    "        \"iteraciones\": 200,\n",
    "        \"log\": 10,\n",
    "        \"dict_size\": 1000,\n",
    "    },\n",
    ")\n",
    "\n",
    "base_dir = f\"dict-{conf['dict_size']}_its-{conf['iteraciones']}_a-{conf['a']}_b-{conf['b']}_d-{conf['d']}_opt-{conf['opt']}_lamda-{conf['lamd']}_tbs-{conf['train_b_s']}\"\n",
    "ODL = OnlineDictionaryLearning(\n",
    "    luisa,\n",
    "    test_batch_size=1000,\n",
    "    base_dir=base_dir,\n",
    "    log_step=conf[\"log\"],\n",
    ")\n",
    "D = ODL.learn(\n",
    "    it=conf[\"iteraciones\"],\n",
    "    lam=conf[\"lamd\"],\n",
    "    k=conf[\"dict_size\"],\n",
    "    optimizer=conf[\"opt\"],\n",
    "    init_A_mod=conf[\"a\"],\n",
    "    init_B_mod=conf[\"b\"],\n",
    "    init_D_mod=conf[\"d\"],\n",
    "    train_batch_size=conf[\"train_b_s\"],\n",
    ")\n",
    "gif_name = f\"proceso_diccionario_its-{conf['iteraciones']}_lam-{conf['lamd']}_k-{conf['dict_size']}_{conf['opt']}_A-{conf['a']}_B-{conf['b']}_D-{conf['d']}.gif\"\n",
    "paths.append(f\"{base_dir}/{gif_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
